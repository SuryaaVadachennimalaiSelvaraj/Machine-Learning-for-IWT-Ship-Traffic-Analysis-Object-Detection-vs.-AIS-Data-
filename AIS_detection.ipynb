{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SuryaaVadachennimalaiSelvaraj/Machine-Learning-for-IWT-Ship-Traffic-Analysis-Object-Detection-vs.-AIS-Data-/blob/main/AIS_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installing Dependencies\n",
        "The following dependencies are required to run our code"
      ],
      "metadata": {
        "id": "6m07QNN2sMIV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8fajIKVH3xf",
        "outputId": "e4894956-0671-4dba-9630-037bbb7ada39"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'yolov5'...\n",
            "remote: Enumerating objects: 15874, done.\u001b[K\n",
            "remote: Counting objects: 100% (106/106), done.\u001b[K\n",
            "remote: Compressing objects: 100% (86/86), done.\u001b[K\n",
            "remote: Total 15874 (delta 45), reused 56 (delta 20), pack-reused 15768\u001b[K\n",
            "Receiving objects: 100% (15874/15874), 14.68 MiB | 14.44 MiB/s, done.\n",
            "Resolving deltas: 100% (10857/10857), done.\n",
            "/content/yolov5\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.5/188.5 kB\u001b[0m \u001b[31m912.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m606.2/606.2 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.4/57.4 kB\u001b[0m \u001b[31m626.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.3/155.3 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.8/67.8 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m59.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Setup complete. Using torch 2.0.1+cu118 (CPU)\n"
          ]
        }
      ],
      "source": [
        "#clone YOLOv5 and\n",
        "!git clone https://github.com/ultralytics/yolov5  # clone repo\n",
        "%cd yolov5\n",
        "%pip install -qr requirements.txt # install dependencies\n",
        "%pip install -q roboflow\n",
        "!pip install torch torchvision\n",
        "!pip install opencv-python-headless\n",
        "!pip install ultralytics\n",
        "!pip install pandas matplotlib\n",
        "\n",
        "import torch\n",
        "import os\n",
        "from IPython.display import Image, clear_output  # to display images\n",
        "\n",
        "print(f\"Setup complete. Using torch {torch.__version__} ({torch.cuda.get_device_properties(0).name if torch.cuda.is_available() else 'CPU'})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset\n",
        "\n",
        "The dataset is imported from Roboflow"
      ],
      "metadata": {
        "id": "x2SwYRM8sY_j"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TGYKb63WIn73",
        "outputId": "281d4afe-b8bc-4a45-8028-d63f0daeadd1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: roboflow in /usr/local/lib/python3.10/dist-packages (1.1.2)\n",
            "Requirement already satisfied: certifi==2022.12.7 in /usr/local/lib/python3.10/dist-packages (from roboflow) (2022.12.7)\n",
            "Requirement already satisfied: chardet==4.0.0 in /usr/local/lib/python3.10/dist-packages (from roboflow) (4.0.0)\n",
            "Requirement already satisfied: cycler==0.10.0 in /usr/local/lib/python3.10/dist-packages (from roboflow) (0.10.0)\n",
            "Requirement already satisfied: idna==2.10 in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.10)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.4.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from roboflow) (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.22.4)\n",
            "Requirement already satisfied: opencv-python>=4.1.2 in /usr/local/lib/python3.10/dist-packages (from roboflow) (4.7.0.72)\n",
            "Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from roboflow) (8.4.0)\n",
            "Requirement already satisfied: pyparsing==2.4.7 in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.8.2)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.0.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.27.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.16.0)\n",
            "Requirement already satisfied: supervision in /usr/local/lib/python3.10/dist-packages (from roboflow) (0.12.0)\n",
            "Requirement already satisfied: urllib3>=1.26.6 in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.26.16)\n",
            "Requirement already satisfied: wget in /usr/local/lib/python3.10/dist-packages (from roboflow) (3.2)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from roboflow) (4.65.0)\n",
            "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from roboflow) (6.0.1)\n",
            "Requirement already satisfied: requests-toolbelt in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.0.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (1.1.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (4.41.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (23.1)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->roboflow) (2.0.12)\n",
            "Requirement already satisfied: opencv-python-headless<5.0.0.0,>=4.8.0.74 in /usr/local/lib/python3.10/dist-packages (from supervision->roboflow) (4.8.0.74)\n",
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n",
            "Downloading Dataset Version Zip in Inland-vessel-5 to yolov5pytorch: 100% [3570339 / 3570339] bytes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting Dataset Version Zip to Inland-vessel-5 in yolov5pytorch:: 100%|██████████| 182/182 [00:00<00:00, 783.87it/s]\n"
          ]
        }
      ],
      "source": [
        "!pip install roboflow\n",
        "\n",
        "from roboflow import Roboflow\n",
        "rf = Roboflow(api_key=\"5Zj0iS5efLpvaTXWNhR7\")\n",
        "project = rf.workspace(\"thesis-sxxfi\").project(\"inland-vessel\")\n",
        "dataset = project.version(5).download(\"yolov5\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training\n",
        "\n",
        "The next step is to train the yolov5 model, this syntax requires the path to the train.py file and the data.yaml file\n",
        "\n",
        "**Train.py:** This file can be found as part of the yolov5 folder which is cloned in the step above and shifted to the drive to keep the progress stored.\n",
        "\n",
        "**Data.yaml:** This file is found as the part of the dataset that has been imported from roboflow. The file is to be edited, once opened you should provide it with the paths of the test, train and valid files from the datset and then save it. This file's path is then provided as input to the syntax"
      ],
      "metadata": {
        "id": "1L-Ctplv2vtU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GR6-2-6QKoeO",
        "outputId": "fb016759-0801-4561-86a3-f1725709a24a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31m\u001b[1mrequirements:\u001b[0m YOLOv5 requirement \"gitpython\" not found, attempting AutoUpdate...\n",
            "Collecting gitpython\n",
            "  Downloading GitPython-3.1.32-py3-none-any.whl (188 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 188.5/188.5 kB 4.9 MB/s eta 0:00:00\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython)\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 62.7/62.7 kB 8.0 MB/s eta 0:00:00\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython)\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: smmap, gitdb, gitpython\n",
            "Successfully installed gitdb-4.0.10 gitpython-3.1.32 smmap-5.0.0\n",
            "\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m 1 package updated per ['gitpython']\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m ⚠️ \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n",
            "\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov5s.pt, cfg=, data=/content/drive/MyDrive/Colab Notebooks/yolov5/Inland-vessel-5/data.yaml, hyp=drive/MyDrive/Colab Notebooks/yolov5/data/hyps/hyp.scratch-low.yaml, epochs=150, batch_size=16, imgsz=416, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, bucket=, cache=ram, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=drive/MyDrive/Colab Notebooks/yolov5/runs/train, name=exp, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n",
            "Command 'git fetch origin' timed out after 5 seconds\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m /content/drive/MyDrive/Colab Notebooks/requirements.txt not found, check failed.\n",
            "fatal: cannot change to '/content/drive/MyDrive/Colab': No such file or directory\n",
            "YOLOv5 🚀 2023-5-3 Python-3.10.12 torch-2.0.1+cu118 CPU\n",
            "\n",
            "remote: Enumerating objects: 178, done.\u001b[K\n",
            "remote: Counting objects: 100% (139/139), done.\u001b[K\n",
            "remote: Compressing objects: 100% (32/32), done.\u001b[K\n",
            "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
            "\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLOv5 🚀 in ClearML\n",
            "\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 🚀 runs in Comet\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir drive/MyDrive/Colab Notebooks/yolov5/runs/train', view at http://localhost:6006/\n",
            "remote: Total 178 (delta 114), reused 122 (delta 107), pack-reused 39\u001b[K\n",
            "Receiving objects: 100% (178/178), 154.83 KiB | 2.31 MiB/s, done.\n",
            "Downloading https://ultralytics.com/assets/Arial.ttf to /root/.config/Ultralytics/Arial.ttf...\n",
            "100% 755k/755k [00:00<00:00, 16.4MB/s]\n",
            "Downloading https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5s.pt to yolov5s.pt...\n",
            "100% 14.1M/14.1M [00:00<00:00, 80.0MB/s]\n",
            "\n",
            "Overriding model.yaml nc=80 with nc=1\n",
            "\n",
            "                 from  n    params  module                                  arguments                     \n",
            "  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n",
            "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
            "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
            "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
            "  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n",
            "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
            "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
            "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
            "  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n",
            "  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n",
            " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
            " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
            " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
            " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
            " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
            " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
            " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
            " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
            " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
            " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
            " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
            " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
            " 24      [17, 20, 23]  1     16182  models.yolo.Detect                      [1, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n",
            "Model summary: 214 layers, 7022326 parameters, 7022326 gradients\n",
            "\n",
            "Transferred 343/349 items from yolov5s.pt\n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.0005), 60 bias\n",
            "Resolving deltas: 100% (121/121), completed with 45 local objects.\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drive/MyDrive/Colab Notebooks/yolov5/Inland-vessel-5/train/labels.cache... 72 images, 9 backgrounds, 0 corrupt: 100% 72/72 [00:00<?, ?it/s]\n",
            "From https://github.com/ultralytics/yolov5\n",
            "   3812a1a..dd10481  master     -> origin/master\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/Colab Notebooks/yolov5/train.py\", line 642, in <module>\n",
            "    main(opt)\n",
            "  File \"/content/drive/MyDrive/Colab Notebooks/yolov5/train.py\", line 531, in main\n",
            "    train(opt.hyp, opt, device, callbacks)\n",
            "  File \"/content/drive/MyDrive/Colab Notebooks/yolov5/train.py\", line 190, in train\n",
            "    train_loader, dataset = create_dataloader(train_path,\n",
            "  File \"/content/drive/MyDrive/Colab Notebooks/yolov5/utils/dataloaders.py\", line 124, in create_dataloader\n",
            "    dataset = LoadImagesAndLabels(\n",
            "  File \"/content/drive/MyDrive/Colab Notebooks/yolov5/utils/dataloaders.py\", line 571, in __init__\n",
            "    if cache_images == 'ram' and not self.check_cache_ram(prefix=prefix):\n",
            "  File \"/content/drive/MyDrive/Colab Notebooks/yolov5/utils/dataloaders.py\", line 595, in check_cache_ram\n",
            "    im = cv2.imread(random.choice(self.im_files))  # sample image\n",
            "  File \"/content/drive/MyDrive/Colab Notebooks/yolov5/utils/general.py\", line 1140, in imread\n",
            "    return cv2.imdecode(np.fromfile(filename, np.uint8), flags)\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "!python /content/drive/MyDrive/Colab\\ Notebooks/yolov5/train.py --img 416 --batch 16 --epochs 150 --data /content/drive/MyDrive/Colab\\ Notebooks/yolov5/Inland-vessel-5/data.yaml --weights yolov5s.pt --cache"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Detection:\n",
        "\n",
        "Next step is to test the detection of the model,\n",
        "\n",
        "* The syntax requires the path to the weights file that's attained as the result of training the model.\n",
        "    1.   It is found in Yolov5 ➡ runs ➡ train ➡ exp ➡ weights ➡ **best.pt**\n",
        "\n",
        "*   The Path to the detect.py file in the Yolov5 folder\n",
        "*   The path to the video source is to be provided, this is foud in the yolov5 ➡ data ➡ **Videos**.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lBQSfadw5QjV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OnL0kTHnpYxg",
        "outputId": "72e8e8b2-7030-41a7-d6ee-ae493c5f6973"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['/content/drive/MyDrive/Colab Notebooks/yolov5/runs/train/exp/weights/best.pt'], source=/content/drive/MyDrive/Colab Notebooks/yolov5/data/Videos/rhine.mp4, data=drive/MyDrive/Colab Notebooks/yolov5/data/coco128.yaml, imgsz=[640, 640], conf_thres=0.4, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=drive/MyDrive/Colab Notebooks/yolov5/runs/detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False, vid_stride=1\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m /content/drive/MyDrive/Colab Notebooks/requirements.txt not found, check failed.\n",
            "fatal: cannot change to '/content/drive/MyDrive/Colab': No such file or directory\n",
            "YOLOv5 🚀 2023-5-3 Python-3.10.11 torch-2.0.0+cu118 CPU\n",
            "\n",
            "Fusing layers... \n",
            "Model summary: 157 layers, 7012822 parameters, 0 gradients\n",
            "video 1/1 (1/9150) /content/drive/MyDrive/Colab Notebooks/yolov5/data/Videos/rhine.mp4: 384x640 1 Ship, 257.5ms\n",
            "video 1/1 (2/9150) /content/drive/MyDrive/Colab Notebooks/yolov5/data/Videos/rhine.mp4: 384x640 1 Ship, 233.7ms\n",
            "video 1/1 (3/9150) /content/drive/MyDrive/Colab Notebooks/yolov5/data/Videos/rhine.mp4: 384x640 1 Ship, 236.3ms\n",
            "video 1/1 (4/9150) /content/drive/MyDrive/Colab Notebooks/yolov5/data/Videos/rhine.mp4: 384x640 1 Ship, 231.7ms\n",
            "video 1/1 (5/9150) /content/drive/MyDrive/Colab Notebooks/yolov5/data/Videos/rhine.mp4: 384x640 1 Ship, 240.0ms\n",
            "video 1/1 (6/9150) /content/drive/MyDrive/Colab Notebooks/yolov5/data/Videos/rhine.mp4: 384x640 1 Ship, 213.6ms\n",
            "video 1/1 (7/9150) /content/drive/MyDrive/Colab Notebooks/yolov5/data/Videos/rhine.mp4: 384x640 1 Ship, 225.9ms\n",
            "video 1/1 (8/9150) /content/drive/MyDrive/Colab Notebooks/yolov5/data/Videos/rhine.mp4: 384x640 1 Ship, 210.2ms\n",
            "video 1/1 (9/9150) /content/drive/MyDrive/Colab Notebooks/yolov5/data/Videos/rhine.mp4: 384x640 1 Ship, 214.0ms\n",
            "video 1/1 (10/9150) /content/drive/MyDrive/Colab Notebooks/yolov5/data/Videos/rhine.mp4: 384x640 1 Ship, 215.1ms\n",
            "video 1/1 (11/9150) /content/drive/MyDrive/Colab Notebooks/yolov5/data/Videos/rhine.mp4: 384x640 1 Ship, 221.2ms\n",
            "video 1/1 (12/9150) /content/drive/MyDrive/Colab Notebooks/yolov5/data/Videos/rhine.mp4: 384x640 1 Ship, 207.0ms\n",
            "video 1/1 (13/9150) /content/drive/MyDrive/Colab Notebooks/yolov5/data/Videos/rhine.mp4: 384x640 1 Ship, 216.1ms\n",
            "video 1/1 (14/9150) /content/drive/MyDrive/Colab Notebooks/yolov5/data/Videos/rhine.mp4: 384x640 1 Ship, 224.8ms\n",
            "video 1/1 (15/9150) /content/drive/MyDrive/Colab Notebooks/yolov5/data/Videos/rhine.mp4: 384x640 1 Ship, 216.0ms\n",
            "video 1/1 (16/9150) /content/drive/MyDrive/Colab Notebooks/yolov5/data/Videos/rhine.mp4: 384x640 1 Ship, 221.2ms\n",
            "video 1/1 (17/9150) /content/drive/MyDrive/Colab Notebooks/yolov5/data/Videos/rhine.mp4: 384x640 1 Ship, 213.7ms\n",
            "video 1/1 (18/9150) /content/drive/MyDrive/Colab Notebooks/yolov5/data/Videos/rhine.mp4: 384x640 1 Ship, 224.8ms\n",
            "video 1/1 (19/9150) /content/drive/MyDrive/Colab Notebooks/yolov5/data/Videos/rhine.mp4: 384x640 1 Ship, 207.7ms\n",
            "video 1/1 (20/9150) /content/drive/MyDrive/Colab Notebooks/yolov5/data/Videos/rhine.mp4: 384x640 1 Ship, 227.3ms\n",
            "video 1/1 (21/9150) /content/drive/MyDrive/Colab Notebooks/yolov5/data/Videos/rhine.mp4: 384x640 1 Ship, 224.3ms\n",
            "video 1/1 (22/9150) /content/drive/MyDrive/Colab Notebooks/yolov5/data/Videos/rhine.mp4: 384x640 1 Ship, 229.7ms\n",
            "video 1/1 (23/9150) /content/drive/MyDrive/Colab Notebooks/yolov5/data/Videos/rhine.mp4: 384x640 1 Ship, 224.6ms\n",
            "video 1/1 (24/9150) /content/drive/MyDrive/Colab Notebooks/yolov5/data/Videos/rhine.mp4: 384x640 1 Ship, 215.4ms\n",
            "video 1/1 (25/9150) /content/drive/MyDrive/Colab Notebooks/yolov5/data/Videos/rhine.mp4: 384x640 1 Ship, 220.2ms\n",
            "video 1/1 (26/9150) /content/drive/MyDrive/Colab Notebooks/yolov5/data/Videos/rhine.mp4: 384x640 1 Ship, 222.0ms\n",
            "video 1/1 (27/9150) /content/drive/MyDrive/Colab Notebooks/yolov5/data/Videos/rhine.mp4: 384x640 1 Ship, 231.5ms\n",
            "video 1/1 (28/9150) /content/drive/MyDrive/Colab Notebooks/yolov5/data/Videos/rhine.mp4: 384x640 1 Ship, 219.2ms\n",
            "video 1/1 (29/9150) /content/drive/MyDrive/Colab Notebooks/yolov5/data/Videos/rhine.mp4: 384x640 1 Ship, 222.9ms\n",
            "video 1/1 (30/9150) /content/drive/MyDrive/Colab Notebooks/yolov5/data/Videos/rhine.mp4: 384x640 1 Ship, 232.0ms\n",
            "video 1/1 (31/9150) /content/drive/MyDrive/Colab Notebooks/yolov5/data/Videos/rhine.mp4: 384x640 1 Ship, 231.6ms\n",
            "video 1/1 (32/9150) /content/drive/MyDrive/Colab Notebooks/yolov5/data/Videos/rhine.mp4: 384x640 1 Ship, 212.7ms\n",
            "video 1/1 (33/9150) /content/drive/MyDrive/Colab Notebooks/yolov5/data/Videos/rhine.mp4: 384x640 1 Ship, 219.4ms\n",
            "video 1/1 (34/9150) /content/drive/MyDrive/Colab Notebooks/yolov5/data/Videos/rhine.mp4: 384x640 1 Ship, 222.8ms\n",
            "video 1/1 (35/9150) /content/drive/MyDrive/Colab Notebooks/yolov5/data/Videos/rhine.mp4: 384x640 1 Ship, 233.1ms\n",
            "video 1/1 (36/9150) /content/drive/MyDrive/Colab Notebooks/yolov5/data/Videos/rhine.mp4: 384x640 1 Ship, 227.6ms\n",
            "video 1/1 (37/9150) /content/drive/MyDrive/Colab Notebooks/yolov5/data/Videos/rhine.mp4: 384x640 1 Ship, 224.8ms\n",
            "video 1/1 (38/9150) /content/drive/MyDrive/Colab Notebooks/yolov5/data/Videos/rhine.mp4: 384x640 1 Ship, 237.7ms\n",
            "video 1/1 (39/9150) /content/drive/MyDrive/Colab Notebooks/yolov5/data/Videos/rhine.mp4: 384x640 1 Ship, 354.1ms\n",
            "video 1/1 (40/9150) /content/drive/MyDrive/Colab Notebooks/yolov5/data/Videos/rhine.mp4: 384x640 1 Ship, 360.8ms\n",
            "video 1/1 (41/9150) /content/drive/MyDrive/Colab Notebooks/yolov5/data/Videos/rhine.mp4: 384x640 1 Ship, 359.6ms\n",
            "video 1/1 (42/9150) /content/drive/MyDrive/Colab Notebooks/yolov5/data/Videos/rhine.mp4: 384x640 1 Ship, 369.0ms\n",
            "video 1/1 (43/9150) /content/drive/MyDrive/Colab Notebooks/yolov5/data/Videos/rhine.mp4: 384x640 1 Ship, 358.1ms\n",
            "video 1/1 (44/9150) /content/drive/MyDrive/Colab Notebooks/yolov5/data/Videos/rhine.mp4: 384x640 1 Ship, 370.9ms\n",
            "video 1/1 (45/9150) /content/drive/MyDrive/Colab Notebooks/yolov5/data/Videos/rhine.mp4: 384x640 1 Ship, 366.3ms\n",
            "video 1/1 (46/9150) /content/drive/MyDrive/Colab Notebooks/yolov5/data/Videos/rhine.mp4: 384x640 1 Ship, 357.5ms\n",
            "video 1/1 (47/9150) /content/drive/MyDrive/Colab Notebooks/yolov5/data/Videos/rhine.mp4: 384x640 1 Ship, 354.4ms\n",
            "video 1/1 (48/9150) /content/drive/MyDrive/Colab Notebooks/yolov5/data/Videos/rhine.mp4: 384x640 1 Ship, 373.2ms\n",
            "video 1/1 (49/9150) /content/drive/MyDrive/Colab Notebooks/yolov5/data/Videos/rhine.mp4: 384x640 1 Ship, 356.1ms\n",
            "video 1/1 (50/9150) /content/drive/MyDrive/Colab Notebooks/yolov5/data/Videos/rhine.mp4: 384x640 1 Ship, 325.6ms\n",
            "video 1/1 (51/9150) /content/drive/MyDrive/Colab Notebooks/yolov5/data/Videos/rhine.mp4: 384x640 1 Ship, 228.1ms\n",
            "video 1/1 (52/9150) /content/drive/MyDrive/Colab Notebooks/yolov5/data/Videos/rhine.mp4: 384x640 1 Ship, 224.2ms\n",
            "video 1/1 (53/9150) /content/drive/MyDrive/Colab Notebooks/yolov5/data/Videos/rhine.mp4: 384x640 1 Ship, 230.4ms\n",
            "video 1/1 (54/9150) /content/drive/MyDrive/Colab Notebooks/yolov5/data/Videos/rhine.mp4: 384x640 1 Ship, 227.6ms\n",
            "video 1/1 (55/9150) /content/drive/MyDrive/Colab Notebooks/yolov5/data/Videos/rhine.mp4: 384x640 1 Ship, 218.4ms\n",
            "video 1/1 (56/9150) /content/drive/MyDrive/Colab Notebooks/yolov5/data/Videos/rhine.mp4: 384x640 1 Ship, 219.6ms\n",
            "video 1/1 (57/9150) /content/drive/MyDrive/Colab Notebooks/yolov5/data/Videos/rhine.mp4: 384x640 1 Ship, 214.4ms\n",
            "video 1/1 (58/9150) /content/drive/MyDrive/Colab Notebooks/yolov5/data/Videos/rhine.mp4: 384x640 1 Ship, 214.6ms\n",
            "video 1/1 (59/9150) /content/drive/MyDrive/Colab Notebooks/yolov5/data/Videos/rhine.mp4: 384x640 1 Ship, 232.3ms\n",
            "video 1/1 (60/9150) /content/drive/MyDrive/Colab Notebooks/yolov5/data/Videos/rhine.mp4: 384x640 1 Ship, 220.0ms\n",
            "video 1/1 (61/9150) /content/drive/MyDrive/Colab Notebooks/yolov5/data/Videos/rhine.mp4: 384x640 1 Ship, 216.0ms\n",
            "video 1/1 (62/9150) /content/drive/MyDrive/Colab Notebooks/yolov5/data/Videos/rhine.mp4: 384x640 1 Ship, 221.1ms\n",
            "video 1/1 (63/9150) /content/drive/MyDrive/Colab Notebooks/yolov5/data/Videos/rhine.mp4: 384x640 1 Ship, 232.6ms\n",
            "video 1/1 (64/9150) /content/drive/MyDrive/Colab Notebooks/yolov5/data/Videos/rhine.mp4: 384x640 1 Ship, 221.8ms\n",
            "video 1/1 (65/9150) /content/drive/MyDrive/Colab Notebooks/yolov5/data/Videos/rhine.mp4: 384x640 1 Ship, 224.7ms\n",
            "video 1/1 (66/9150) /content/drive/MyDrive/Colab Notebooks/yolov5/data/Videos/rhine.mp4: 384x640 1 Ship, 220.5ms\n",
            "video 1/1 (67/9150) /content/drive/MyDrive/Colab Notebooks/yolov5/data/Videos/rhine.mp4: 384x640 1 Ship, 214.1ms\n",
            "video 1/1 (68/9150) /content/drive/MyDrive/Colab Notebooks/yolov5/data/Videos/rhine.mp4: 384x640 1 Ship, 229.4ms\n",
            "video 1/1 (69/9150) /content/drive/MyDrive/Colab Notebooks/yolov5/data/Videos/rhine.mp4: 384x640 1 Ship, 227.0ms\n",
            "video 1/1 (70/9150) /content/drive/MyDrive/Colab Notebooks/yolov5/data/Videos/rhine.mp4: 384x640 1 Ship, 216.3ms\n",
            "video 1/1 (71/9150) /content/drive/MyDrive/Colab Notebooks/yolov5/data/Videos/rhine.mp4: 384x640 1 Ship, 225.2ms\n",
            "video 1/1 (72/9150) /content/drive/MyDrive/Colab Notebooks/yolov5/data/Videos/rhine.mp4: 384x640 1 Ship, 225.7ms\n",
            "video 1/1 (73/9150) /content/drive/MyDrive/Colab Notebooks/yolov5/data/Videos/rhine.mp4: 384x640 1 Ship, 215.7ms\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/Colab Notebooks/yolov5/detect.py\", line 261, in <module>\n",
            "    main(opt)\n",
            "  File \"/content/drive/MyDrive/Colab Notebooks/yolov5/detect.py\", line 256, in main\n",
            "    run(**vars(opt))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/content/drive/MyDrive/Colab Notebooks/yolov5/detect.py\", line 128, in run\n",
            "    pred = model(im, augment=augment, visualize=visualize)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/content/drive/MyDrive/Colab Notebooks/yolov5/models/common.py\", line 515, in forward\n",
            "    y = self.model(im, augment=augment, visualize=visualize) if augment or visualize else self.model(im)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/content/drive/MyDrive/Colab Notebooks/yolov5/models/yolo.py\", line 209, in forward\n",
            "    return self._forward_once(x, profile, visualize)  # single-scale inference, train\n",
            "  File \"/content/drive/MyDrive/Colab Notebooks/yolov5/models/yolo.py\", line 121, in _forward_once\n",
            "    x = m(x)  # run\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/content/drive/MyDrive/Colab Notebooks/yolov5/models/common.py\", line 59, in forward_fuse\n",
            "    return self.act(self.conv(x))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\", line 463, in forward\n",
            "    return self._conv_forward(input, self.weight, self.bias)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\", line 459, in _conv_forward\n",
            "    return F.conv2d(input, weight, bias, self.stride,\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "!python /content/drive/MyDrive/Colab\\ Notebooks/yolov5/detect.py --weights /content/drive/MyDrive/Colab\\ Notebooks/yolov5/runs/train/exp/weights/best.pt --img-size 640 --conf 0.4 --source /content/drive/MyDrive/Colab\\ Notebooks/yolov5/data/Videos/rhine.mp4"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "geqKI8mBTWO_",
        "outputId": "7e73079f-11a8-4cfc-93fa-6ff54ce674f8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code\n",
        "\n",
        " This code  is to detect, count and compare the data to the AIS data file\n",
        "\n",
        " The Input data that is to be provided are:\n",
        "\n",
        "\n",
        "*   Video Source:  yolov5 ➡ data ➡ **Videos**.\n",
        "*   AIS data - Here uploaded into the Yolov5 file as __AIS data.json__ and the path fof the same file is uploaded\n",
        "*   Start time of the video (Format: \"2023-05-31 14:00:00\")\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HbT13040HSoA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s9ixaizhq-F2",
        "outputId": "7f019571-b365-4cdb-a049-c5237172ffd6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/ultralytics_yolov5_master\n",
            "YOLOv5 🚀 2023-8-31 Python-3.10.12 torch-2.0.1+cu118 CPU\n",
            "\n",
            "Fusing layers... \n",
            "Model summary: 157 layers, 7012822 parameters, 0 gradients\n",
            "Adding AutoShape... \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of frames: 907.0\n",
            "New ship found: {'centroid': (395.5, 233.0), 'direction': None, 'counted': False}+++++++++++++\n",
            "\n",
            "New ship found: {'centroid': (612.5, 220.0), 'direction': None, 'counted': False}+++++++++++++\n",
            "\n",
            "Direction assigned: Right to Left for {'centroid': (612.5, 220.0), 'direction': 'Right to Left', 'counted': False}\n",
            "\n",
            "Ship co-ordinates: (511, 192, 707, 247)\n",
            "\n",
            "Coordinates of the matched ship: (511, 192, 707, 247)\n",
            "\n",
            "Ship crossed the line from Right to Left: {'centroid': (609.0, 219.5), 'direction': 'Right to Left', 'counted': True}\n",
            "\n",
            "\n",
            "Time of crossing the line: 2.6666666666666665 seconds\n",
            "\n",
            "Ship crossed the line from Left to Right: {'centroid': (609.0, 219.5), 'direction': 'Right to Left', 'counted': True, 'counted_frame': 80, 'time': 2.6666666666666665}\n",
            "\n",
            "\n",
            "New ship found: {'centroid': (693.0, 263.5), 'direction': None, 'counted': False}+++++++++++++\n",
            "\n",
            "Direction assigned: Left to Right for {'centroid': (693.0, 263.5), 'direction': 'Left to Right', 'counted': False}\n",
            "\n",
            "Coordinates of the matched ship: (560, 202, 847, 329)\n",
            "\n",
            "Time of crossing the line: 18.666666666666668 seconds\n",
            "\n",
            "Ship crossed the line from Left to Right: {'centroid': (703.5, 265.5), 'direction': 'Left to Right', 'counted': True, 'counted_frame': 560, 'time': 18.666666666666668}\n",
            "\n",
            "\n",
            "New ship found: {'centroid': (511.5, 211.5), 'direction': None, 'counted': False}+++++++++++++\n",
            "\n",
            "New ship found: {'centroid': (767.0, 273.0), 'direction': None, 'counted': False}+++++++++++++\n",
            "\n",
            "New ship found: {'centroid': (433.0, 215.5), 'direction': None, 'counted': False}+++++++++++++\n",
            "\n",
            "Direction assigned: Left to Right for {'centroid': (433.0, 215.5), 'direction': 'Left to Right', 'counted': False}\n",
            "\n",
            "Left to Right: 1\n",
            "Right to Left: 1\n",
            "Total number of frames: 45\n",
            "\n",
            "\n",
            "{'centroid': (605.5, 221.0), 'direction': 'Right to Left', 'counted': True, 'counted_frame': 80, 'time': '2023-05-31 14:00:02'}\n",
            "{'centroid': (726.0, 267.5), 'direction': 'Left to Right', 'counted': True, 'counted_frame': 560, 'time': '2023-05-31 14:00:18'}\n",
            "Total number of matches: 0\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "import json\n",
        "import pandas as pd\n",
        "import sys\n",
        "import datetime\n",
        "\n",
        "\n",
        "# Load YOLO and import my weights\n",
        "model = torch.hub.load('ultralytics/yolov5', 'custom', path=\"/content/drive/MyDrive/Colab Notebooks/yolov5/runs/train/exp/weights/best.pt\")\n",
        "\n",
        "# Load video\n",
        "cap = cv2.VideoCapture(\"/content/drive/MyDrive/Colab Notebooks/yolov5/data/Videos/Multiple ships.mp4\")\n",
        "\n",
        "# get json file path from command line argument\n",
        "json_file = \"/content/drive/MyDrive/Colab Notebooks/yolov5/AIS data.json\"\n",
        "\n",
        "\n",
        "# input the start time of the video in the format YYYY-MM-DD HH:MM:SS\n",
        "start_time = \"2023-05-31 14:00:00\"\n",
        "\n",
        "# Initialize a dictionary that saves previous ship's centroids, Direction and counted status\n",
        "previous_ships = []\n",
        "\n",
        "def calculate_centroid(box):\n",
        "    x1, y1, x2, y2 = box\n",
        "    centroid_x = (x1 + x2) / 2\n",
        "    centroid_y = (y1 + y2) / 2\n",
        "    return (centroid_x, centroid_y)\n",
        "\n",
        "def calculate_distance(centroid1, centroid2):\n",
        "    x1, y1 = centroid1\n",
        "    x2, y2 = centroid2\n",
        "    return np.sqrt((x2 - x1)**2 + (y2 - y1)**2)\n",
        "\n",
        "\n",
        "# Set minimum confidence threshold for detections\n",
        "conf_thresh = 0.75\n",
        "\n",
        "# Reference line for counting for right to left direction\n",
        "ref_line = 800\n",
        "\n",
        "# Define centroid distance threshold for ship matching\n",
        "centroid_distance_threshold = 25\n",
        "\n",
        "#initialize a parameter for the number of frames to skip\n",
        "skip_frames = 20\n",
        "\n",
        "#Initialize counts\n",
        "left_to_right_count = 0\n",
        "right_to_left_count = 0\n",
        "\n",
        "#Initialize frame count\n",
        "count_frames = 0\n",
        "#print total number of frames in the video\n",
        "print(\"Total number of frames: \" + str(cap.get(cv2.CAP_PROP_FRAME_COUNT)))\n",
        "\n",
        "FPS = cap.get(cv2.CAP_PROP_FPS)\n",
        "\n",
        "while True:\n",
        "    # Read frame from video\n",
        "    ret, frame = cap.read()\n",
        "\n",
        "    # Stop if end of video or if its a blank frame\n",
        "    if frame is None:\n",
        "        break\n",
        "\n",
        "    # Count the frame number\n",
        "    frame_number = int(cap.get(cv2.CAP_PROP_POS_FRAMES))\n",
        "\n",
        "   # process every other frame\n",
        "    if frame_number % skip_frames != 0:\n",
        "        continue\n",
        "\n",
        "    # Count the frames that are processed\n",
        "    count_frames = count_frames + 1\n",
        "\n",
        "    # Pass frame through YOLOv5s model\n",
        "    detections = model(frame)\n",
        "\n",
        "    # get detections with confidence higher than conf_thresh\n",
        "    detections = detections.pred[0][detections.pred[0][:, 4] > conf_thresh]\n",
        "\n",
        "    # loop through detections\n",
        "    for detection in detections:\n",
        "        # get confidence score and class index\n",
        "        confidence = detection[4]\n",
        "        class_index = int(detection[5])\n",
        "\n",
        "        # chech if the confidence is higher than the threshold\n",
        "        if confidence > conf_thresh:\n",
        "            # get bounding box coordinates\n",
        "            x1, y1, x2, y2 = detection[:4].detach().numpy().astype(np.int32)\n",
        "\n",
        "            #centroid of the bounding box\n",
        "            centroid_x,centroid_y = calculate_centroid((x1, y1, x2, y2))\n",
        "\n",
        "            # Check if the current centroid is near any previous centroid only then consider it as the same ship\n",
        "            matched_ship = None\n",
        "            previous_centroid = None\n",
        "            for ship in previous_ships:\n",
        "                previous_centroid = ship[\"centroid\"]\n",
        "                if isinstance(previous_centroid, np.ndarray):\n",
        "                    for centroid in previous_centroid:\n",
        "                        distance = calculate_distance(centroid, (centroid_x, centroid_y))\n",
        "                        if distance < centroid_distance_threshold:\n",
        "                            previous_centroid = centroid\n",
        "                            matched_ship = ship\n",
        "                            break\n",
        "                else:\n",
        "                    distance = calculate_distance(previous_centroid, (centroid_x, centroid_y))\n",
        "                    if distance < centroid_distance_threshold:\n",
        "                        matched_ship = ship\n",
        "                        break\n",
        "\n",
        "\n",
        "            # Also check if direction is not assigned and save the direction based on the previous centroid centre point, even if the difference is small\n",
        "            if matched_ship is not None and matched_ship[\"direction\"] is None:\n",
        "                prev_centroid_x, prev_centroid_y = previous_centroid\n",
        "                if centroid_x > prev_centroid_x and centroid_y > prev_centroid_y:\n",
        "                    matched_ship[\"direction\"] = \"Left to Right\"\n",
        "                    print(\"Direction assigned: Left to Right for \" + str(matched_ship) + \"\\n\")\n",
        "                    #print co-ordinates of the ship\n",
        "                    #print(\"Ship co-ordinates: \" + str((x1, y1, x2, y2)) + \"\\n\")\n",
        "\n",
        "                elif centroid_x < prev_centroid_x and centroid_y < prev_centroid_y:\n",
        "                    matched_ship[\"direction\"] = \"Right to Left\"\n",
        "                    print(\"Direction assigned: Right to Left for \" + str(matched_ship) + \"\\n\")\n",
        "                    print(\"Ship co-ordinates: \" + str((x1, y1, x2, y2)) + \"\\n\")\n",
        "\n",
        "           # Update the centroid of the matched ship if its relatively close to the previous centroid\n",
        "            if matched_ship is not None:\n",
        "                if isinstance(previous_centroid, np.ndarray):\n",
        "                    for centroid in previous_centroid:\n",
        "                        if calculate_distance(centroid, (centroid_x, centroid_y)) < centroid_distance_threshold:\n",
        "                            matched_ship[\"centroid\"] = (centroid_x, centroid_y)\n",
        "                            break\n",
        "                else:\n",
        "                    matched_ship[\"centroid\"] = (centroid_x, centroid_y)\n",
        "\n",
        "            # If no match found, create a new ship\n",
        "            if matched_ship is None:\n",
        "                matched_ship = {\"centroid\": (centroid_x, centroid_y), \"direction\": None, \"counted\": False}\n",
        "                # Print values of the new ship\n",
        "                print(\"New ship found: \" + str(matched_ship) + \"+++++++++++++\\n\")\n",
        "                previous_ships.append(matched_ship)\n",
        "\n",
        "          #Check if the box border is crossing the line and cross check with the direction of the ship\n",
        "            if matched_ship[\"direction\"] == \"Right to Left\" and x1 < ref_line and not matched_ship[\"counted\"]:\n",
        "                # print values of the matched ship\n",
        "                right_to_left_count += 1\n",
        "                print(\"Coordinates of the matched ship: \" + str((x1, y1, x2, y2)) + \"\\n\")\n",
        "                matched_ship[\"counted\"] = True\n",
        "                print(\"Ship crossed the line from Right to Left: \" + str(matched_ship) + \"\\n\\n\")\n",
        "                # Also based on FPS and current frame number, note the time of crossing the line\n",
        "                match_time = (frame_number / cap.get(cv2.CAP_PROP_FPS))\n",
        "                matched_ship[\"counted_frame\"] = frame_number\n",
        "                print(\"Time of crossing the line: \" + str(match_time) + \" seconds\\n\")\n",
        "                matched_ship[\"time\"] = match_time\n",
        "\n",
        "                print(\"Ship crossed the line from Left to Right: \" + str(matched_ship) + \"\\n\\n\")\n",
        "\n",
        "            elif matched_ship[\"direction\"] == \"Left to Right\" and x2 > ref_line and not matched_ship[\"counted\"]:\n",
        "                # print values of the matched ship\n",
        "                left_to_right_count += 1\n",
        "                #print coordinates of the matched ship in the frame\n",
        "                print(\"Coordinates of the matched ship: \" + str((x1, y1, x2, y2)) + \"\\n\")\n",
        "                matched_ship[\"counted\"] = True\n",
        "                # Also based on FPS and current frame number, note the time of crossing the line\n",
        "                match_time = (frame_number / cap.get(cv2.CAP_PROP_FPS))\n",
        "                matched_ship[\"counted_frame\"] = frame_number\n",
        "                print(\"Time of crossing the line: \" + str(match_time) + \" seconds\\n\")\n",
        "                matched_ship[\"time\"] = match_time\n",
        "\n",
        "                print(\"Ship crossed the line from Left to Right: \" + str(matched_ship) + \"\\n\\n\")\n",
        "\n",
        "\n",
        "            # Draw bounding box to fit the detected object\n",
        "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "\n",
        "            # Draw centroids for ships\n",
        "            cv2.circle(frame, (int(centroid_x), int(centroid_y)), 5, (0, 255, 0), -1)\n",
        "\n",
        "    # Draw reference line for counting\n",
        "    cv2.line(frame, (ref_line, 0), (ref_line, 720), (0, 0, 255), 2)\n",
        "\n",
        "    # Display counts on frame\n",
        "    cv2.putText(frame, \"Left to Right: \" + str(left_to_right_count), (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 255), 2)\n",
        "    cv2.putText(frame, \"Right to Left: \" + str(right_to_left_count), (10, 100), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 255), 2)\n",
        "\n",
        "    # # Display frame\n",
        "    # cv2.imshow(\"YOLOv5\", frame)\n",
        "\n",
        "    # Press Q on keyboard to exit\n",
        "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
        "        break\n",
        "\n",
        "# When everything done, release the video capture object\n",
        "cap.release()\n",
        "\n",
        "# Closes all the frames\n",
        "cv2.destroyAllWindows()\n",
        "\n",
        "# Print counts\n",
        "print(\"Left to Right: \" + str(left_to_right_count))\n",
        "print(\"Right to Left: \" + str(right_to_left_count))\n",
        "print(\"Total number of frames: \" + str(count_frames) + \"\\n\\n\")\n",
        "\n",
        "# based on the start time of the video, calculate the time of crossing the line for each ship\n",
        "start_time = datetime.datetime.strptime(start_time, \"%Y-%m-%d %H:%M:%S\")\n",
        "for ship in previous_ships:\n",
        "    if ship[\"counted\"]:\n",
        "        # Find the time of crossing the line and save as string in the ship dictionary in format YYYY-MM-DD HH:MM:SS\n",
        "        time_of_crossing = start_time + datetime.timedelta(seconds=ship[\"time\"])\n",
        "        ship[\"time\"] = time_of_crossing.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "        print(ship)\n",
        "\n",
        "# Load JSON file\n",
        "with open(json_file) as f:\n",
        "    json_data = json.load(f)\n",
        "\n",
        "# Convert JSON to DataFrame\n",
        "df = pd.DataFrame(json_data)\n",
        "\n",
        "# Convert timestamp to datetime\n",
        "df['timeLastUpdate'] = pd.to_datetime(df['timeLastUpdate'], unit='ms')\n",
        "\n",
        "# Sort DataFrame by timestamp in ascending order\n",
        "df = df.sort_values('timeLastUpdate')\n",
        "\n",
        "# add a column to the DataFrame to save the matched ship(yes/no)\n",
        "df[\"Matched ship\"] = \"\"\n",
        "\n",
        "# create a column to the dataframe to save the number of matches per range or timestamp\n",
        "df[\"Number of matches\"] = \"\"\n",
        "# assign 0 to all the entries in the column\n",
        "df[\"Number of matches\"] = 0\n",
        "# make the varable type as int\n",
        "df[\"Number of matches\"] = df[\"Number of matches\"].astype(int)\n",
        "\n",
        "\n",
        "# add a column to save the timelasupdate as a range of values\n",
        "df[\"'timeLastUpdate': ['first', 'last']\"] = \"\"\n",
        "\n",
        "# if multiple entries with the same mmsi, keep one entry with the timestamp value being the range of the first and last timestamp, enter this value in the new column\n",
        "# After a range is obtained for a ship, delete all other entries with the same mmsi\n",
        "for index, row in df.iterrows():\n",
        "    if df[df[\"mmsi\"] == row[\"mmsi\"]].shape[0] > 1:\n",
        "        df.at[index, \"'timeLastUpdate': ['first', 'last']\"] = {\"first\": df[df[\"mmsi\"] == row[\"mmsi\"]][\"timeLastUpdate\"].iloc[0], \"last\": df[df[\"mmsi\"] == row[\"mmsi\"]][\"timeLastUpdate\"].iloc[-1]}\n",
        "        df.drop(df[df[\"mmsi\"] == row[\"mmsi\"]].index[1:], inplace=True)\n",
        "\n",
        "\n",
        "\n",
        "# Try to mach the previous_ships list by comparing the time of crossing the line with the timeLastUpdate\n",
        "for ship in previous_ships:\n",
        "    for index, row in df.iterrows():\n",
        "        if ship[\"counted\"]:\n",
        "            # convert timeLastUpdate to datetime\n",
        "            timeLastUpdate = row[\"timeLastUpdate\"]\n",
        "            # Convert time of crossing the line to datetime\n",
        "            time_of_crossing = datetime.datetime.strptime(ship[\"time\"], \"%Y-%m-%d %H:%M:%S\")\n",
        "            # print(\"Time of crossing the line: \" + str(time_of_crossing) + \" Time Last Update  \" + str(timeLastUpdate) +   \"\\n\")\n",
        "            # if the time last update is a range of values, check if the time of crossing the line is in that range\n",
        "            if isinstance(row[\"'timeLastUpdate': ['first', 'last']\"], dict):\n",
        "               if time_of_crossing >= row[\"'timeLastUpdate': ['first', 'last']\"][\"first\"] and time_of_crossing <= row[\"'timeLastUpdate': ['first', 'last']\"][\"last\"]:\n",
        "                    df.at[index, \"Matched ship\"] = \"Yes\"\n",
        "                    # increase the number of matches for that range\n",
        "                    df.at[index, \"Number of matches\"] = df.at[index, \"Number of matches\"] + 1\n",
        "                    print(\"Matched ship: \" + str(ship) + \" with \" + str(row[\"mmsi\"]) + \"\\n\")\n",
        "                    break\n",
        "            # if the time last update is not a range of values, check if the time of crossing the line is within 2 minutes of the timeLastUpdate\n",
        "            else:\n",
        "                if time_of_crossing >= timeLastUpdate - datetime.timedelta(minutes=2) and time_of_crossing <= timeLastUpdate + datetime.timedelta(minutes=2):\n",
        "                    df.at[index, \"Matched ship\"] = \"Yes\"\n",
        "                    # increase the number of matches for that range\n",
        "                    df.at[index, \"Number of matches\"] = df.at[index, \"Number of matches\"] + 1\n",
        "                    print(\"Matched ship: \" + str(ship) + \" with \" + str(row[\"mmsi\"]) + \"\\n\")\n",
        "                    break\n",
        "\n",
        "# Also print the sum of the number of matches column in the end of the column\n",
        "print(\"Total number of matches: \" + str(df[\"Number of matches\"].sum()) + \"\\n\\n\")\n",
        "\n",
        "# Save only timeLastUpdate, mmsi column and if the ship is matched to that entry into excel file from the DataFrame and print it to excel and save it in the same folder as the script\n",
        "df.to_excel(\"AIS_data.xlsx\", columns=[\"timeLastUpdate\",\"'timeLastUpdate': ['first', 'last']\", \"mmsi\", \"Matched ship\", \"Number of matches\"], index=False)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "# plot a bargraph of the number of matches per timelasupdate\n",
        "df.plot.bar(x=\"mmsi\", y=\"Number of matches\", rot=90)\n",
        "#decrease the size of the label on x axis\n",
        "plt.xticks(fontsize=5)\n",
        "#make the label of y axis whole numbers instead of decimals\n",
        "plt.yticks(np.arange(0, 10, 1))\n",
        "plt.show()\n",
        "#save the plot as a png file\n",
        "plt.savefig(\"Number of matches per mmsi.png\")\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}